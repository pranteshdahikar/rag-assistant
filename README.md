# RAG Assistant

This project is a Retrieval-Augmented Generation (RAG) assistant built with **LangChain**, **FAISS**, and **Hugging Face** models.  
It lets you query your local documents (`.txt` and `.pdf`) in natural language.

---

## ðŸš€ Installation

## Setup Instructions

1. **Clone the repository**

git clone https://github.com/your-username/rag-assistant.git
cd rag-assistant


2. Create a virtual environment (optional but recommended)

python -m venv venv
# Windows
venv\Scripts\activate

3. Install dependencies
pip install -r requirements.txt

4. Prepare your documents
Place .txt or .json files inside the data/ folder.
JSON files should contain text fields such as title or publication_description.

5. Build the vectorstore
python src/build_vectorstore.py

6. Run the RAG assistant
python src/rag_assistant.py

### Usage Examples

âœ… RAG assistant is ready! Ask questions about your documents.
ðŸ’¡ Tip: Use 'doc:filename.txt your question' to restrict search to a file.
ðŸ’¡ Tip: Use 'memory:<user_id> your question' to track memory per user.


Ask a question (or type 'exit'): What tools are mentioned?
ðŸ”¹ Loading FAISS vectorstore...
ðŸ”¹ Initializing local Hugging Face LLM...
Device set to use cpu

ðŸ“„ Answer:
 Toolboxes

Ask a question (or type 'exit'): Whatâ€™s this publication about?
ðŸ”¹ Loading FAISS vectorstore...
ðŸ”¹ Initializing local Hugging Face LLM...
Device set to use cpu

ðŸ“„ Answer:
 Science/Tech

Ask a question (or type 'exit'): Any limitations or assumptions?
ðŸ”¹ Loading FAISS vectorstore...
ðŸ”¹ Initializing local Hugging Face LLM...
Device set to use cpu

ðŸ“„ Answer:
 A section that discusses the limitations of the dataset and the Mini-RAG system, addressing any potential issues, trade-offs, and the impact of any limitations on the research outcomes.


### How It Works
1.	Documents are loaded and split into chunks.
2.	Each chunk is embedded using Hugging Face embeddings.
3.	A FAISS vectorstore is created for fast similarity search.
4.	User queries are sent through a RetrievalQA chain:
â€¢	Retrieves top-k relevant chunks
â€¢	Uses a local Hugging Face LLM to generate answers
â€¢	Optional: Filters retrieval by document source
â€¢	Optional: Uses past questions to generate standalone queries

## Notes
â€¢	Use exit or quit to stop the assistant.
â€¢	JSON documents must include meaningful text fields for embeddings.
â€¢	You can change the model in src/rag_assistant.py if desired.
â€¢	The system prompt for modifying questions can be customized.
â€¢	For memory-enabled RAG, use MongoDB or other database integration.



## Dependencies:

â€¢	Python 3.10+
â€¢	langchain
â€¢	langchain-huggingface
â€¢	transformers
â€¢	faiss-cpu
â€¢	python-dotenv (optional, if using OpenAI API)
â€¢	pymongo (optional, for memory storage)
